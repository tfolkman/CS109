{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Exponential distribution\n",
      "    * Memoryless - things don't decay; they are always good as new. There is no progress; so if avg. wait time for bus is 10 and distributed\n",
      "        expontentially, then no matter when you arrive, expected wait time is 10 minutes\n",
      "    * Might use even if not exactly right because might have some of the properties you want\n",
      "    * Also could be the starting place, and make changes as find errors - data science is an iterative process"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Length Biasing Paradox\n",
      "    * The bus example above\n",
      "    * Prison sentence - what is avg prison sentence? Might go take survey of prisoners and take avg of results. That would be wrong because \n",
      "        would miss short prison sentences (those who were only there for a few days and left before you took your sample)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Weibull Distribution\n",
      "    * Used a lot for survival distributions; generalizes the problem of memoryless with the exponential dist\n",
      "    * Hazard function - instantanous prob. of death given survived to that point in time; for exponential it is a constant - not realistic\n",
      "    * Weibull adds another parameter - takes the exponential to the power of this additional parameter"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Cauchy distribution\n",
      "    * Heavier tail than normal\n",
      "    * Ratio of two independent normals"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Binomial Distribution\n",
      "    * The number of success in n independent bernoulli(p) trials\n",
      "    * Mean = np; E(x+y) = E(x) + E(y) -> always true\n",
      "    * Variance = np(1-p); if independent or uncorrelated, Var(X+Y) = Var(X) + Var(Y)\n",
      "    * Looks like normal when N is large -> really works best when p = 1/2 because symmetric -> otherwise so skewed that N needs to be huge\n",
      "    "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Normal Distribution\n",
      "    * Symmetry\n",
      "    * Central Limit Therom - tells us that if we take the mean of the samples of size s (n) and plot the frequencies of their mean, we get a normal  distribution! And as the sample size (n) increases --> approaches infinity, we find a normal distribution.\n",
      "    * 68-95-99.7 rule\n",
      "    * Maximizes entropy (disorder) for a given mean variance"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Bootstrap\n",
      "    * Idea: Make some more datasets by resampling with replacement from the data we do have\n",
      "    * Compute estimator of interest for each bootstrap data set\n",
      "    * Then use these data to make histograms and compute CI\n",
      "    * Basically what we are doing is using the law of large numbers to create empirical distributions to approximate the actual distribution"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}